{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2db4ea9ce6bed60c3ffa0cf5ff09b0d4efbb589"
      },
      "cell_type": "markdown",
      "source": "# Amateur Hour - Using Headlines to Predict Stocks\n### Starter Kernel by ``Magichanics`` \n*([Gitlab](https://gitlab.com/Magichanics) - [Kaggle](https://www.kaggle.com/magichanics))*\n\nStocks are unpredictable, but can sometimes follow a trend. In this notebook, we will be discovering the correlation between the stocks and the news.\n\nIf there are any things that you would like me to add or remove, feel free to comment down below. I'm mainly doing this to learn and experiment with the data. I plan on rewriting a lot of code in the future to make it look nicer, since a lot of the stuff I have written may not be the most efficient way to approach specific problems.\n\n**To Do List:**\n* Groupby count of each ``assetName``/``assetCode``?\n* Use text processing to predict ``universe``?\n* Removing features with low importance.\n\n**What's new?**\n* Added quant features (see https://www.kaggle.com/youhanlee/simple-quant-features-using-python)\n"
    },
    {
      "metadata": {
        "_uuid": "1e268288023fb7fc142e9f9c1c70f2d04c7cc103"
      },
      "cell_type": "markdown",
      "source": "![title](https://upload.wikimedia.org/wikipedia/commons/8/8d/Wall_Street_sign_banner.jpg)\n\nSource: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Wall_Street_sign_banner.jpg)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74ac86edc14f5f49372114111d24ba7f9c095d97",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# main\nimport numpy as np\nimport pandas as pd\nimport os\nfrom itertools import chain\nimport gc\n\n# text processing\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# clustering\nfrom sklearn.cluster import KMeans\n\n# time\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime\n\n# training\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n# import environment for data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62bc61f1400a9c5efeb20a460be9ac2066784169"
      },
      "cell_type": "code",
      "source": "sampling = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aabe093aa9598bd90bd7bec571df3e8cbdfb8d99"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()\n\nif sampling:\n    market_train_df = market_train_df.tail(40_000)\n    news_train_df = news_train_df.tail(100_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07732b5c4888f5de18aa4c424267715dbed5783f"
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "997773dbe4987fbec07c28775c4343acda7f9521"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb841bc6ab0199143c83428d5419ba9a28eec13b"
      },
      "cell_type": "markdown",
      "source": "### Information on the Training Data\n* There are no Unknown ``assetName`` in ``news_train_df``, but there are 24 479 rows with Unknown as the ``assetName`` in ``market_train_df``. Merging by ``assetCode`` leaves out Unknown rows, which could be problematic.\n* ``Volume`` has the highest correlation in terms of ``returnsOpenNextMktres10``.\n* Merging by just ``assetCodes`` greatly increases the dataframe (with just 100k rows, it has turned into 10 million rows), although merging by ``assetCodes`` and ``time`` greatly decrease the original dataframe."
    },
    {
      "metadata": {
        "_uuid": "4e79b71ec8de5009a497f5ad71bf80be76e78329"
      },
      "cell_type": "markdown",
      "source": "### Aggregations on News Data\n\nIt helped a lot during the Home Credit competition, and in the next block of code we will be merging the news dataframe with the market dataframe. Instead of having columns with a list of numbers, we will get aggregations for each grouping. The following block creates a dictionary that will be used when merging the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e45bcca68ec56ffb6d1e3d2049aa4970f6897da4"
      },
      "cell_type": "code",
      "source": "news_agg_cols = [f for f in news_train_df.columns if 'novelty' in f or\n                'volume' in f or\n                'sentiment' in f or\n                'bodySize' in f or\n                'Count' in f or\n                'marketCommentary' in f or\n                'relevance' in f]\nnews_agg_dict = {}\nfor col in news_agg_cols:\n    news_agg_dict[col] = ['mean', 'sum', 'max', 'min']\nnews_agg_dict['urgency'] = ['min', 'count']\nnews_agg_dict['takeSequence'] = ['max']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c1185cd336552dfbc3972772b6cc1ad908654b43"
      },
      "cell_type": "markdown",
      "source": "### Joining Market & News Data\n\nThe grouping method that I'll be using is from [bguberfain](https://www.kaggle.com/bguberfain), but I'll also be adding in the headlines column, as well eliminating rows that are not partnered with either the market or news data. One way I would improve this is probably group by time periods rather than exact times given in ``time`` due to the small amount of data that share the same amount of data in terms of the ``time`` column, and possibly making it a bit more efficient. \n\nNotes: \n* When you run the full dataset, expect it to take a while.\n* As you remove more time features from seconds to year, the resulting train data becomes larger and larger."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2dad49559a057808e6d2559a6fbac4f964bd6d4"
      },
      "cell_type": "code",
      "source": "# note to self: fill int/float columns with 0\ndef fillnulls(X):\n    \n    # fill headlines with the string null\n    X['headline'] = X['headline'].fillna('null')\n    \ndef generalize_time(X):\n    # convert time to string and/or get rid of Hours, Minutes, and seconds\n    X['time'] = X['time'].dt.strftime('%Y-%m-%d %H:%M:%S').str.slice(0,16) #(0,10) for Y-m-d, (0,13) for Y-m-d H\n\n# get dataframes within indecies\ndef get_indecies(df, indecies):\n    \n    # update market dataframe to only contain the specific rows with matching indecies.\n    def check_index(index, indecies):\n        if index in indecies:\n            return True\n        else:\n            return False\n    \n    df['del_index'] = df.index.values\n    df['is_in_indecies'] = df['del_index'].apply(lambda x: check_index(x, indecies))\n    df = df[df.is_in_indecies == True]\n    del df['del_index'], df['is_in_indecies']\n    \n    return df\n\n# this function checks for potential nulls after grouping by only grouping the time and assetcode dataframe\n# returns valid news indecies for the next if statement.\ndef partial_groupby(market_df, news_df, df_assetCodes):\n    \n    # get new dataframe\n    temp_news_df_expanded = pd.merge(df_assetCodes, news_df[['time', 'assetCodes']], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # groupby dataframes\n    temp_news_df = temp_news_df_expanded.copy()[['time', 'assetCode']]\n    temp_market_df = market_df.copy()[['time', 'assetCode']]\n\n    # get indecies on both dataframes\n    temp_news_df['news_index'] = temp_news_df.index.values\n    temp_market_df['market_index'] = temp_market_df.index.values\n\n    # set multiindex and join the two\n    temp_news_df.set_index(['time', 'assetCode'], inplace=True)\n\n    # join the two\n    temp_market_df_2 = temp_market_df.join(temp_news_df, on=['time', 'assetCode'])\n    del temp_market_df, temp_news_df\n\n    # drop nulls in any columns\n    temp_market_df_2 = temp_market_df_2.dropna()\n\n    # get indecies\n    market_valid_indecies = temp_market_df_2['market_index'].tolist()\n    news_valid_indecies = temp_market_df_2['news_index'].tolist()\n    del temp_market_df_2\n\n    # get index rows\n    market_df = get_indecies(market_df, market_valid_indecies)\n    \n    return market_df, news_valid_indecies\n\ndef join_market_news(market_df, news_df, nulls=False):\n    \n    # convert time to string\n    generalize_time(market_df)\n    generalize_time(news_df)\n    \n    # Fix asset codes (str -> list)\n    news_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_df['assetCodes']))\n    assetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\n    \n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    \n    if not nulls:\n        market_df, news_valid_indecies = partial_groupby(market_df, news_df, df_assetCodes)\n    \n    # create dataframe based on groupby\n    news_col = ['time', 'assetCodes', 'headline'] + sorted(list(news_agg_dict.keys()))\n    news_df_expanded = pd.merge(df_assetCodes, news_df[news_col], left_on='level_0', right_index=True, suffixes=(['','_old']))\n    \n    # check if the columns are in the index\n    if not nulls:\n        news_df_expanded = get_indecies(news_df_expanded, news_valid_indecies)\n\n    def news_df_feats(x):\n        if x.name == 'headline':\n            return list(x)\n    \n    # groupby time and assetcode\n    news_df_expanded = news_df_expanded.reset_index()\n    news_groupby = news_df_expanded.groupby(['time', 'assetCode'])\n    \n    # get aggregated df\n    news_df_aggregated = news_groupby.agg(news_agg_dict).apply(np.float32).reset_index()\n    news_df_aggregated.columns = ['_'.join(col).strip() for col in news_df_aggregated.columns.values]\n    \n    # get any important string dataframes\n    news_df_cat = news_groupby.transform(lambda x: news_df_feats(x))['headline'].to_frame()\n    new_news_df = pd.concat([news_df_aggregated, news_df_cat], axis=1)\n    \n    # cleanup\n    del news_df_aggregated\n    del news_df_cat\n    del news_df\n    \n    # rename columns\n    new_news_df.rename(columns={'time_': 'time', 'assetCode_': 'assetCode'}, inplace=True)\n    new_news_df.set_index(['time', 'assetCode'], inplace=True)\n    \n    # Join with train\n    market_df = market_df.join(new_news_df, on=['time', 'assetCode'])\n\n    # cleanup\n    fillnulls(market_df)\n\n    return market_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2be1b6e851438405ca972ddf6fdcc6ea8c97cd28",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "%%time\nX_train = join_market_news(market_train_df, news_train_df, nulls=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d38b9ffd46a21e96bc55b142452eb7cec432c68"
      },
      "cell_type": "code",
      "source": "X_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67e32e082a729f9aad122d3bcb492a6707e021ea"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "957f29e020695f32628882848eecdf392aa9e9e0"
      },
      "cell_type": "markdown",
      "source": "### Text Processing with Logistic Regression\n\nWe are going to vectorize the headlines and apply logistic regression (labels being binary as to whether the stocks go up or not). In a nutshell, it splits the headlines into individual words, filters out unecessary words to prevent abnormal results, vectorizes it for modelling, and then with the target column provided, we could create a dataframe of coefficients that we could use as a feature in the dataframe! Right now I am just getting the mean of the coefficients in each list of headlines. \n\nNote: May be useful to apply it to ``universe``, and possibly get the sum or standard deviation of the word coefficients?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d25c279b7face9bb58eb15a2780a7ec7e9d6b6f8"
      },
      "cell_type": "code",
      "source": "# reuse data\ndef round_scores(x):\n    if x >= 0:\n        return 1\n    else:\n        return 0\n    \ndef clean_headlines(headline):\n    \n    # remove numerical and convert to lowercase\n    headline =  re.sub('[^a-zA-Z]',' ',headline)\n    headline = headline.lower()\n    \n    # drop stopwords\n    headline_words = headline.split(' ')\n    headline_words = [word for word in headline_words if not word in stopwords.words('english')]\n    \n    # use stemming to simplify words\n    ps = PorterStemmer()\n    headline_words = [ps.stem(word) for word in headline_words]\n    \n    # join sentence back again\n    return ' '.join(headline_words)\n\n# these functions should only go towards the training data only\ndef get_headline_df(X_train):\n    \n    headlines_lst = []\n    target_lst = []\n    \n    # iter through every headline.\n    for row in range(0,len(X_train.index)):\n        for sentence in X_train['headline'].iloc[row]:\n            headlines_lst.append(clean_headlines(sentence))\n            target_lst.append(round_scores(X_train['returnsOpenNextMktres10'].iloc[row]))\n            \n    # return dataframe\n    return pd.DataFrame({'headline':pd.Series(headlines_lst), 'returnsOpenNextMktres10':pd.Series(target_lst)})\n    \ndef get_headline(headlines_df):\n    \n    # get headlines as list (use only headline_df produced by get_headline_df)\n    headlines_lst = []\n    for row in range(0,len(headlines_df.index)):\n        headlines_lst.append(headlines_df.iloc[row])\n\n    # split headlines to separate words\n    basicvectorizer = CountVectorizer()\n    headlines_vectorized = basicvectorizer.fit_transform(headlines_lst)\n    \n    print(headlines_vectorized.shape)\n    return headlines_vectorized, basicvectorizer\n\ndef headline_mapping(target, headlines_vectored, headline_vectorizer):\n    \n    print(np.asarray(target).shape)\n    headline_model = LogisticRegression()\n    headline_model = headline_model.fit(headlines_vectored, target)\n    \n    # get coefficients\n    basicwords = headline_vectorizer.get_feature_names()\n    basiccoeffs = headline_model.coef_.tolist()[0]\n    coeff_df = pd.DataFrame({'Word' : basicwords, \n                            'Coefficient' : basiccoeffs})\n    \n    # convert dataframe to dictionary of coefficients\n    coefficient_dict = dict(zip(coeff_df.Word, coeff_df.Coefficient))\n\n    return coefficient_dict, coeff_df['Coefficient'].mean()\n\n# for predictions\ndef get_coeff_col(X, coeff_dict, coeff_default):\n    \n    def get_coeff(word_lst):\n        \n        # iter through every word\n        coeff_sum = 0\n        for word in word_lst:\n            if word in coeff_dict:\n                coeff_sum += coeff_dict[word]\n            else:\n                coeff_sum += coeff_default\n        \n        # get average coefficient\n        coeff_score = coeff_sum / len(word_lst)\n        return coeff_score\n        \n    basicvectorizer = CountVectorizer()\n    \n    # loop through every item\n    headlines_coeff_lst = []\n    for row in range(0,len(X['headline'].index)):\n        coeff_score = 0\n        for i in range(0,len(X['headline'].iloc[row])):\n            coeff_score += get_coeff(clean_headlines(str(X['headline'].iloc[row][i])).split(' '))\n        headlines_coeff_lst.append(coeff_score / len(X['headline'].iloc[row]))\n        \n    # merge coefficient frame with main\n    coeff_mean_df = pd.DataFrame({'headline_coeff_mean': pd.Series(headlines_coeff_lst)})\n    X = pd.concat([X.reset_index(), coeff_mean_df], axis=1)\n    \n    return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b58b721978bceccdf5ae8048defa3064c9babbb"
      },
      "cell_type": "code",
      "source": "headline_df = get_headline_df(X_train)\ncoefficient_dict, coefficient_default = headline_mapping(headline_df['returnsOpenNextMktres10'],\n                                            *get_headline(headline_df['headline']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40f4b1fa7424613014e3e53a39087315c947b82e"
      },
      "cell_type": "code",
      "source": "# will be applied to X_test as well\nX_train = get_coeff_col(X_train, coefficient_dict, coefficient_default)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "033aca8966e7cc4eb8030dafab9e693b4866457d"
      },
      "cell_type": "markdown",
      "source": "### Clustering\nWe are going to be clustering a few columns together (mainly to see how this will affect our results). "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a269f60c0dc9defa2dfc04c5680b99b5a43b031b"
      },
      "cell_type": "code",
      "source": "def clustering(df):\n\n    def cluster_modelling(features):\n        df_set = df[features]\n        cluster_model = KMeans(n_clusters = 8)\n        cluster_model.fit(df_set)\n        return cluster_model.predict(df_set)\n    \n    # get columns:\n    vol_cols = [f for f in df.columns if f != 'volume' and 'volume' in f]\n    novelty_cols = [f for f in df.columns if 'novelty' in f]\n    \n    # fill nulls\n    cluster_cols = novelty_cols + vol_cols + ['open', 'close']\n    df[cluster_cols] = df[cluster_cols].fillna(0)\n    \n    df['cluster_open_close'] = cluster_modelling(['open', 'close'])\n    df['cluster_volume'] = cluster_modelling(vol_cols)\n    df['cluster_novelty'] = cluster_modelling(novelty_cols)\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "563074af5ae1dab8c5584afef865e8a0d8eef686"
      },
      "cell_type": "code",
      "source": "X_train = clustering(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bdad0b0f9d641b5e028548285cd64747dd5a8ff"
      },
      "cell_type": "markdown",
      "source": "### Extra Features\n\nHere are some basic extra features from other notebooks, including [Moving averages from YouHan Lee](https://www.kaggle.com/youhanlee/simple-quant-features-using-python)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03532b463b9b4a083aea991708fda777334fbfe2"
      },
      "cell_type": "code",
      "source": "def extra_features(df):\n    \n    # Adding daily difference\n    new_col = df[\"close\"] - df[\"open\"]\n    df.insert(loc=6, column=\"daily_diff\", value=new_col)\n    df['close_to_open'] =  np.abs(df['close'] / df['open'])\n    \n    # source of moving average features: https://www.kaggle.com/youhanlee/simple-quant-features-using-python\n    # moving average\n    df['MA_7MA'] = df['close'].rolling(window=7).mean()\n    df['MA_15MA'] = df['close'].rolling(window=15).mean()\n    df['MA_30MA'] = df['close'].rolling(window=30).mean()\n    df['MA_60MA'] = df['close'].rolling(window=60).mean()\n    \n    # exponential moving average\n    ewma = pd.Series.ewm\n    df['close_30EMA'] = ewma(df[\"close\"], span=30).mean()\n    \n    # MACD\n    df['close_26EMA'] = ewma(df[\"close\"], span=26).mean()\n    df['close_12EMA'] = ewma(df[\"close\"], span=12).mean()\n    df['MACD'] = df['close_12EMA'] - df['close_26EMA']\n    \n    # Bollinger Band\n    no_of_std = 2\n    df['MA_7MA'] = df['close'].rolling(window=7).mean()\n    df['MA_7MA_std'] = df['close'].rolling(window=7).std() \n    df['MA_7MA_BB_high'] = df['MA_7MA'] + no_of_std * df['MA_7MA_std']\n    df['MA_7MA_BB_low'] = df['MA_7MA'] - no_of_std * df['MA_7MA_std']\n    df['MA_15MA'] = df['close'].rolling(window=15).mean()\n    df['MA_15MA_std'] = df['close'].rolling(window=15).std() \n    df['MA_15MA_BB_high'] = df['MA_15MA'] + no_of_std * df['MA_15MA_std']\n    df['MA_15MA_BB_low'] = df['MA_15MA'] - no_of_std * df['MA_15MA_std']\n    df['MA_30MA'] = df['close'].rolling(window=30).mean()\n    df['MA_30MA_std'] = df['close'].rolling(window=30).std() \n    df['MA_30MA_BB_high'] = df['MA_30MA'] + no_of_std * df['MA_30MA_std']\n    df['MA_30MA_BB_low'] = df['MA_30MA'] - no_of_std * df['MA_30MA_std']\n    \n    # volume moving average\n    df['VMA_7MA'] = df['volume'].rolling(window=7).mean()\n    df['VMA_15MA'] = df['volume'].rolling(window=15).mean()\n    df['VMA_30MA'] = df['volume'].rolling(window=30).mean()\n    df['VMA_60MA'] = df['volume'].rolling(window=60).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f83952b1d7852568b8f1d86630d1d8286d90871d"
      },
      "cell_type": "code",
      "source": "extra_features(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b6bc63a91be4bd007fb95c728aadd14a870358ce"
      },
      "cell_type": "markdown",
      "source": "### Get Time Features\n\nThis section splits the timestamp column into their own separate columns, as well as other various time features.\n\nPossible idea: Encoding time?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83c73101057e4aa8c72784ab3fb3c3b497677fc1"
      },
      "cell_type": "code",
      "source": "# ripped from my previous kernel, NYC Taxi Fare\n\n# first get dates\ndef split_time(df):\n    \n    # split date_time into categories\n    df['time_day'] = df['time'].str.slice(8,10)\n    df['time_month'] = df['time'].str.slice(5,7)\n    df['time_year'] = df['time'].str.slice(0,4)\n    df['time_hour'] = df['time'].str.slice(11,13)\n    df['time_minute'] = df['time'].str.slice(14,16)\n    \n    # source: https://www.kaggle.com/nicapotato/taxi-rides-time-analysis-and-oof-lgbm\n    df['temp_time'] = df['time'].str.replace(\" UTC\", \"\")\n    df['temp_time'] = pd.to_datetime(df['temp_time'], format='%Y-%m-%d %H')\n    \n    df['time_day_of_year'] = df.temp_time.dt.dayofyear\n    df['time_week_of_year'] = df.temp_time.dt.weekofyear\n    df[\"time_weekday\"] = df.temp_time.dt.weekday\n    df[\"time_quarter\"] = df.temp_time.dt.quarter\n    \n    del df['temp_time']\n    gc.collect()\n    \n    # convert to non-object columns\n    time_feats = ['time_day', 'time_month', 'time_year']\n    df[time_feats] = df[time_feats].apply(pd.to_numeric)\n    \n    # determine whether the day is set on a holiday\n    cal = USFederalHolidayCalendar()\n    holidays = cal.holidays(start='2007-01-01', end='2018-09-27').to_pydatetime()\n    df['on_holiday'] = df['time'].str.slice(0,10).apply(lambda x: 1 if x in holidays else 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6b15afa722c354bc34bd5c747060687cdd061be"
      },
      "cell_type": "code",
      "source": "split_time(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "674bc842d3ab32df25a88facf5a7af2bfdd3254d"
      },
      "cell_type": "markdown",
      "source": "### Cleaning Data\nRemoves all categorical data as well as data that does not show up in the test data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7da8cedce5276b9c145c59ddeeef423df45200b"
      },
      "cell_type": "code",
      "source": "def remove_cols(X):\n    del_cols = [f for f in X.columns if X[f].dtype == 'object'] + ['assetName', 'index']\n    for f in del_cols:\n        del X[f]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "007ed8fc74313b6f41789369c765f0293b4d6142"
      },
      "cell_type": "code",
      "source": "remove_cols(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f19d6c49c01767a6b015b7bdca8ee090308978d1"
      },
      "cell_type": "markdown",
      "source": "### Compile X functions into one function\n\nThis will be used when looping through different batches of X_test"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6d8cd8162bb0275302e10bbe5399f87a027fe00"
      },
      "cell_type": "code",
      "source": "def get_X(market_df, news_df):\n    \n    # these are all the functions applied to X_train except for a few\n    X_test = join_market_news(market_df, news_df, nulls=True)\n    X_test = get_coeff_col(X_test, coefficient_dict, coefficient_default)\n    X_test = clustering(X_test)\n    extra_features(X_test)\n    split_time(X_test)\n    remove_cols(X_test)\n    \n    return X_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "055fef0b89c100b52ec1e6405ca5e3dde68314e8"
      },
      "cell_type": "markdown",
      "source": "#### Resulting Dataframe and Data Correlation to Target column\nWe have went to roughly 50 columns to 135!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ac267c074efb8d7e16620b52320d23ec4eb8b01"
      },
      "cell_type": "code",
      "source": "X_train.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07a88fc50ca5727ed3f299ab2a9ffd4b2ae05e33"
      },
      "cell_type": "markdown",
      "source": "### Using LightGBM for Modelling + Remove unecessary features\n\nWe are going to use parameters from a notebook for modelling our data, as well as looping through the data until we reach a certain score.\n\nNote: Might possibly add bayesian optimization if necessary?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "047e06ca0fcfef2e08814c00763e631fa557c67f"
      },
      "cell_type": "code",
      "source": "def set_data(X_train):\n    \n    # get X and Y\n    y_train = X_train['returnsOpenNextMktres10']\n    del X_train['returnsOpenNextMktres10'], X_train['universe']\n    \n    # split data (for cross validation)\n    x1, x2, y1, y2 = train_test_split(X_train, \n                                      y_train, \n                                      test_size=0.25, \n                                      random_state=99)\n    \n    return x1, x2, y1, y2\n    \ndef lgbm_training(x1, x2, y1, y2):\n    \n    # set model and parameters\n    params = {'learning_rate': 0.02,\n              'boosting': 'gbdt', \n              'objective': 'regression', \n              'seed': 2018}\n    \n    # train data\n    lgb_model = lgb.train(params, \n                            lgb.Dataset(x1, label=y1), \n                            5000, \n                            lgb.Dataset(x2, label=y2), \n                            verbose_eval=100, \n                            early_stopping_rounds=200)\n    \n    return lgb_model\n\n# def filter_features(X_train):\n    \n#     fcols = df.drop('returnsOpenNextMktres10', axis = 1).columns [f for f in X_train.columns if f != 'returnsOpenNextMktres10' or 'universe']\n    \n#     score = 1\n#     while score > 0.3:\n#         lgb_model = lgbm_training(*set_data(X_train))\n#         f_imp = pd.Series(lgb_model.feature_importances_, index = fcols)\n#         print(score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7ac52c7404a316e83acdb297d2d736969ae00db"
      },
      "cell_type": "code",
      "source": "lgb_model = lgbm_training(*set_data(X_train))",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'returnsOpenNextMktres10'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'returnsOpenNextMktres10'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-118c0bfecf4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgbm_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-eb8f6025cfbc>\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(X_train)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# get X and Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'returnsOpenNextMktres10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'returnsOpenNextMktres10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'universe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'returnsOpenNextMktres10'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5e95d79bbf2795a52dcb275da4133e52db8164cc"
      },
      "cell_type": "markdown",
      "source": "#### Feature Importance"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "67a1e43e2212e6cb1292fe49f6be39ed8b6ce87d"
      },
      "cell_type": "code",
      "source": "#lgb_model.feature_importances_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3762332a038544f7ed3c9ba5acd5c6fd114b946f"
      },
      "cell_type": "markdown",
      "source": "### Making Predictions\n\nNow the difference between the training and test data would be these two columns,  ``['returnsOpenNextMktres10', 'universe']``. We will be trying to predict ``returnsOpenNextMktres10`` and using that as the ``confidenceValue``."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1fb894dd1216d1dbf6e48d15d336a8fd1847067"
      },
      "cell_type": "code",
      "source": "%%time\n\ndef make_predictions(market_obs_df, news_obs_df):\n    \n    # predict using given model\n    X_test = get_X(market_obs_df, news_obs_df)\n    prediction_values = np.clip(lgb_model.predict(X_test), -1, 1)\n\n    return prediction_values\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days(): # Looping over days from start of 2017 to 2019-07-15\n    \n    # make predictions\n    predictions_template_df['confidenceValue'] = make_predictions(market_obs_df, news_obs_df)\n    \n    # save predictions\n    env.predict(predictions_template_df)\n",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mmake_predictions\u001b[0;34m(market_obs_df, news_obs_df)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-4be2b530432b>\u001b[0m in \u001b[0;36mget_X\u001b[0;34m(market_df, news_df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# these are all the functions applied to X_train except for a few\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin_market_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnulls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coeff_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficient_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficient_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ed07d0b9e917>\u001b[0m in \u001b[0;36mjoin_market_news\u001b[0;34m(market_df, news_df, nulls)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# get any important string dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mnews_df_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_groupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnews_df_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mnew_news_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnews_df_aggregated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_df_cat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4432\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4434\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4436\u001b[0m         \u001b[0;31m# a reduction transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_transform_general\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4390\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4392\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4394\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   4475\u001b[0m             \u001b[0mfast_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4476\u001b[0m             slow_path = lambda group: group.apply(\n\u001b[0;32m-> 4477\u001b[0;31m                 lambda x: func(x, *args, **kwargs), axis=self.axis)\n\u001b[0m\u001b[1;32m   4478\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfast_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslow_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6012\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6013\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6014\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m                                       *self.args, **self.kwds)\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameRowApply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# as demonstrated in gh-12244\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         if (self.result_type in ['reduce', None] and\n\u001b[0;32m--> 228\u001b[0;31m                 not self.dtypes.apply(is_extension_type).any()):\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;31m# Create a dummy Series from an empty array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3200\u001b[0m             return self._constructor(mapped,\n\u001b[0;32m-> 3201\u001b[0;31m                                      index=self.index).__finalize__(self)\n\u001b[0m\u001b[1;32m   3202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m     def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    275\u001b[0m                                        raise_cast_failure=True)\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   4675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4677\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4679\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m \u001b[0;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_ndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmgr_locs\u001b[0;34m(self, new_mgr_locs)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmgr_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlockPlacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f9c3668a134db935e6a7e266a15a47a7292f537e"
      },
      "cell_type": "markdown",
      "source": "### Export Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02e6a605c9ca804310fbc788b465c226ddfea533"
      },
      "cell_type": "code",
      "source": "# exports csv\nenv.write_submission_file()\nprint('finished!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d8f34f002aa0dc6efda2caf73a8fd5e2724515b"
      },
      "cell_type": "markdown",
      "source": "### References:\n* [Getting Started - DJ Sterling](https://www.kaggle.com/dster/two-sigma-news-official-getting-started-kernel)\n* [a simple model - Bruno G. do Amaral](https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-data)\n* [LGBM Model - the1owl](https://www.kaggle.com/the1owl/my-two-sigma-cents-only)\n* [Headline Processing - Andrew Gel](https://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit)\n* [Feature engineering - Andrew Lukyanenko](https://www.kaggle.com/artgor/eda-feature-engineering-and-everything)\n* [Basic Text Processing - akatsuki06](https://www.kaggle.com/akatsuki06/basic-text-processing-cleaning-the-description)\n* [Simple quant features - YouHan Lee](https://www.kaggle.com/youhanlee/simple-quant-features-using-python)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}