{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2db4ea9ce6bed60c3ffa0cf5ff09b0d4efbb589"
      },
      "cell_type": "markdown",
      "source": "# Amateur Hour - Using Headlines to Predict Stocks\n### Starter Kernel by ``Magichanics`` \n*([Gitlab](https://gitlab.com/Magichanics) - [Kaggle](https://www.kaggle.com/magichanics))*\n\nStocks are unpredictable, but can sometimes follow a trend. In this notebook, we will be discovering the correlation between the stocks and the news. After a few tries, my best score not including this one is ``0.54194`` from [V29](https://www.kaggle.com/magichanics/amateur-hour-using-headlines-to-predict-stocks/code?scriptVersionId=6466412). Right now, I'm trying to find answers as to why my score keeps fluctuating from ``-0.26`` to ``0.55``.  A lot of the scrapped code that I was going to use can be found on my GitLab account. Since the data provided for both the final submissions and training/test, we will be accounting for no new ``subjects``, ``assetCode``, ``assetName`` and ``audiences``.\n\nIf there are any things that you would like me to add or remove, feel free to comment down below. I'm mainly doing this to learn and experiment with the data. I plan on rewriting a lot of code in the future to make it look nicer, since a lot of the stuff I have written may not be the most efficient way to approach specific problems.\n\n*\"News don't affect stock prices, people's reaction do.\"*\n\n**To Do List:**\n* Removing features with low importance?\n* Applying a Neural Network\n* Possibly accounting for new ``assetCode``, ``assetName``, ``subjects`` and ``audiences``?\n\n"
    },
    {
      "metadata": {
        "_uuid": "1e268288023fb7fc142e9f9c1c70f2d04c7cc103"
      },
      "cell_type": "markdown",
      "source": "![title](https://upload.wikimedia.org/wikipedia/commons/8/8d/Wall_Street_sign_banner.jpg)\n\nSource: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Wall_Street_sign_banner.jpg)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74ac86edc14f5f49372114111d24ba7f9c095d97",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# main\nimport numpy as np\nimport pandas as pd\nimport os\nfrom itertools import chain\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# text processing\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# clustering\nfrom sklearn.cluster import KMeans\n\n# time\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime\n\n# training\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n# import environment for data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62bc61f1400a9c5efeb20a460be9ac2066784169"
      },
      "cell_type": "code",
      "source": "sampling = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aabe093aa9598bd90bd7bec571df3e8cbdfb8d99"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()\n\n# its best to get the data by its tail\nif sampling:\n    market_train_df = market_train_df.tail(40_000)\n    news_train_df = news_train_df.tail(100_000)\nelse:\n    market_train_df = market_train_df.tail(1_000_000) # 3m to 6m was too much?\n    news_train_df = news_train_df.tail(4_000_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07732b5c4888f5de18aa4c424267715dbed5783f",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "997773dbe4987fbec07c28775c4343acda7f9521"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb841bc6ab0199143c83428d5419ba9a28eec13b"
      },
      "cell_type": "markdown",
      "source": "### Information on the Training Data\n* There are no Unknown ``assetName`` in ``news_train_df``, but there are 24 479 rows with Unknown as the ``assetName`` in ``market_train_df``. Merging by ``assetCode`` leaves out Unknown rows, which could be problematic.\n* ``Volume`` has the highest correlation in terms of ``returnsOpenNextMktres10``.\n* Merging by just ``assetCodes`` greatly increases the dataframe (with just 100k rows, it has turned into 10 million rows), although merging by ``assetCodes`` and ``time`` greatly decrease the original dataframe."
    },
    {
      "metadata": {
        "_uuid": "dded6df37466545fdcb6f4055b7c309ba21251ac"
      },
      "cell_type": "markdown",
      "source": "### Getting rid of \"Fake\" News\nThere are some things to notice about the news:\n* Some headlines' length have a value of 0. (roughly 300-500 rows?)\n* One Day delayed news (calculated by difference between ``sourceTimestamp`` and ``time``\n* **Experimental** - News with only ``takeSequence`` as 1 (only reported once) and ``urgency`` at 3 (article). They are probably the most unlikely to produce a reaction from the audience if only one news source has talked about it."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ee51debdc9694fcbcb07022f59489a1172b3580"
      },
      "cell_type": "code",
      "source": "def clean_news(news_df):\n    \n    # get rid of blank headlines\n    news_df = news_df[news_df.headline != '']\n    \n    # get rid of delayed news\n    news_df['news_delay'] = news_df['time'] - news_df['sourceTimestamp']\n    news_df = news_df[news_df.news_delay < datetime.timedelta(days=1)]\n    \n    # get rid of headline articles with only 1 takeSequence (experimental)\n    news_df = news_df[(news_df.takeSequence != 1) | (news_df.urgency == 1)]\n    \n    return news_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f401121c908bf6518647e748c00eebdac530ff41"
      },
      "cell_type": "code",
      "source": "news_train_df = clean_news(news_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "070fb26741c73f2bb5c3fb174f5933e1c59718f6"
      },
      "cell_type": "code",
      "source": "news_train_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd8880860fcf020af02370ce7498e8080b255a7f"
      },
      "cell_type": "markdown",
      "source": "### Market Groupby Features\nWe are going to group market data based on ``assetName`` and determine the median and mean of the volume."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff536df56be10e120771990d229091aa308d995d"
      },
      "cell_type": "code",
      "source": "def mean_volume(market_df):\n    \n    # groupby and return median\n    vol_by_name_median = market_df[['volume', 'assetName']].groupby('assetName').median()['volume']\n    vol_by_name_mean = market_df[['volume', 'assetName']].groupby('assetName').mean()['volume'] # could try mean?\n    market_df['vol_by_name_median'] = market_df['assetName'].map(vol_by_name_median)\n    market_df['vol_by_name_mean'] = market_df['assetName'].map(vol_by_name_mean)\n    \n    # get difference for median\n    market_df['vol_by_name_median_diff'] = market_df['volume'] - market_df['vol_by_name_median']\n    \n    return market_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2564cb6e622b3659e7f80351df5f0e3a5b2fbf48"
      },
      "cell_type": "code",
      "source": "market_train_df = mean_volume(market_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c7f9e06bc3c67f74d02ffdf3e60dbeb564ed498a"
      },
      "cell_type": "markdown",
      "source": "### [Simple Quant Features](https://www.kaggle.com/youhanlee/simple-quant-features-using-python)\nPlease go check out YouHan Lee's notebook on Simple Quant Features!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e647155b7c06bb529bb79f2f1a7333c8e59dbaa"
      },
      "cell_type": "code",
      "source": "def quant_feats(market_df):\n    \n    # get moving average\n    market_df['MA_7MA'] = market_df['close'].rolling(window=7).mean()\n    market_df['MA_15MA'] = market_df['close'].rolling(window=15).mean()\n    market_df['MA_30MA'] = market_df['close'].rolling(window=30).mean()\n    market_df['MA_60MA'] = market_df['close'].rolling(window=60).mean()\n    \n    # bollinger band\n    no_of_std = 2\n    market_df['MA_7MA'] = market_df['close'].rolling(window=7).mean()\n    market_df['MA_7MA_std'] = market_df['close'].rolling(window=7).std() \n    market_df['MA_7MA_BB_high'] = market_df['MA_7MA'] + no_of_std * market_df['MA_7MA_std']\n    market_df['MA_7MA_BB_low'] = market_df['MA_7MA'] - no_of_std * market_df['MA_7MA_std']\n    market_df['MA_15MA'] = market_df['close'].rolling(window=15).mean()\n    market_df['MA_15MA_std'] = market_df['close'].rolling(window=15).std() \n    market_df['MA_15MA_BB_high'] = market_df['MA_15MA'] + no_of_std * market_df['MA_15MA_std']\n    market_df['MA_15MA_BB_low'] = market_df['MA_15MA'] - no_of_std * market_df['MA_15MA_std']\n    market_df['MA_30MA'] = market_df['close'].rolling(window=30).mean()\n    market_df['MA_30MA_std'] = market_df['close'].rolling(window=30).std() \n    market_df['MA_30MA_BB_high'] = market_df['MA_30MA'] + no_of_std * market_df['MA_30MA_std']\n    market_df['MA_30MA_BB_low'] = market_df['MA_30MA'] - no_of_std * market_df['MA_30MA_std']\n    \n    # get moving volume average\n    market_df['VMA_7MA'] = market_df['volume'].rolling(window=7).mean()\n    market_df['VMA_15MA'] = market_df['volume'].rolling(window=15).mean()\n    market_df['VMA_30MA'] = market_df['volume'].rolling(window=30).mean()\n    market_df['VMA_60MA'] = market_df['volume'].rolling(window=60).mean()\n    \n    return market_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20c846bad08e701c0da91c544f47fd676d15f0ea"
      },
      "cell_type": "code",
      "source": "market_train_df = quant_feats(market_train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4e79b71ec8de5009a497f5ad71bf80be76e78329"
      },
      "cell_type": "markdown",
      "source": "### Aggregations on News Data\n\nIt helped a lot during the Home Credit competition, and in the next block of code we will be merging the news dataframe with the market dataframe. Instead of having columns with a list of numbers, we will get aggregations for each grouping. The following block creates a dictionary that will be used when merging the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e45bcca68ec56ffb6d1e3d2049aa4970f6897da4"
      },
      "cell_type": "code",
      "source": "news_agg_cols = [f for f in news_train_df.columns if 'novelty' in f or\n                'volume' in f or\n                'sentiment' in f or\n                'bodySize' in f or\n                'Count' in f or\n                'marketCommentary' in f or\n                'relevance' in f]\nnews_agg_dict = {}\nfor col in news_agg_cols:\n    news_agg_dict[col] = ['mean', 'sum', 'max', 'min']\nnews_agg_dict['urgency'] = ['min', 'count']\nnews_agg_dict['takeSequence'] = ['max']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c1185cd336552dfbc3972772b6cc1ad908654b43"
      },
      "cell_type": "markdown",
      "source": "### Joining Market & News Data\n\nThe grouping method that I'll be using is from [bguberfain](https://www.kaggle.com/bguberfain), but I'll also be adding in other columns like ``headline``, as well eliminating rows that are not partnered with either the market or news data. One way I would improve this is probably group by time periods rather than exact times given in ``time`` due to the small amount of data that share the same amount of data in terms of the ``time`` column, and possibly making it a bit more efficient. \n\nNotes: \n* When you run the full dataset, expect it to take a while.\n* As you remove more time features from seconds to year, the resulting train data becomes larger and larger.\n* Add Three Day Moving Period, i.e. combine all data that happened between 3 days before the market time to the market time."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2dad49559a057808e6d2559a6fbac4f964bd6d4"
      },
      "cell_type": "code",
      "source": "def generalize_time(X):\n    # convert time to string and/or get rid of Hours, Minutes, and seconds\n    X['time'] = X['time'].dt.strftime('%Y-%m-%d %H:%M:%S').str.slice(0,13) #(0,10) for Y-m-d, (0,13) for Y-m-d H\n    # do not use (0,10) or (0, 13) on the whole dataset\n    \n# get dataframes within indecies\ndef get_indecies(df, indecies):\n    \n    # update market dataframe to only contain the specific rows with matching indecies.\n    def check_index(index, indecies):\n        if index in indecies:\n            indecies.remove(index)\n            return True\n        else:\n            return False\n    \n    df['del_index'] = df.index.values\n    df['is_in_indecies'] = df['del_index'].apply(lambda x: check_index(x, indecies))\n    df = df[df.is_in_indecies == True]\n    del df['del_index'], df['is_in_indecies']\n    \n    return df\n\ndef add_null_indecies(indecies, valid_indecies, num_nulls):\n    \n    curr_nulls = 0\n    iteration = 0\n    null_indecies = []\n    \n    # print error if its empty\n    if len(indecies) == 0 or len(valid_indecies) == 0:\n        print('No correlation. Try sampling more data!')\n    if num_nulls == 0:\n        return\n    \n    # loop to get any nulls that are not present in the index\n    while curr_nulls < num_nulls and iteration < len(indecies):\n        if indecies[iteration] not in valid_indecies: # filling in missing values i.e. [3, (4), (5), 6, 7...]\n            null_indecies.append(indecies[iteration]) # you could try fitting it between the adjacent values?\n            curr_nulls += 1\n        iteration += 1\n        \n    return null_indecies\n\n# this function checks for potential nulls after grouping by only grouping the time and assetcode dataframe\n# returns valid news indecies for the next if statement.\ndef partial_groupby(market_df, news_df, df_assetCodes, num_nulls):\n    \n    # get new dataframe\n    temp_news_df_expanded = pd.merge(df_assetCodes, news_df[['time', 'assetCodes']], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # groupby dataframes\n    temp_news_df = temp_news_df_expanded.copy()[['time', 'assetCode']]\n    temp_market_df = market_df.copy()[['time', 'assetCode']]\n\n    # get indecies on both dataframes\n    temp_news_df['news_index'] = temp_news_df.index.values\n    temp_market_df['market_index'] = temp_market_df.index.values\n    market_indecies = temp_market_df.index.values.tolist()\n\n    # set multiindex and join the two\n    temp_news_df.set_index(['time', 'assetCode'], inplace=True)\n\n    # join the two\n    temp_market_df_2 = temp_market_df.join(temp_news_df, on=['time', 'assetCode'])\n    del temp_market_df, temp_news_df\n\n    # drop nulls in any columns\n    temp_market_df_2 = temp_market_df_2.dropna()\n\n    # get indecies\n    market_valid_indecies = temp_market_df_2['market_index'].tolist()\n    news_valid_indecies = temp_market_df_2['news_index'].tolist()\n    del temp_market_df_2\n    \n    # get null indecies if stated to do so\n    if num_nulls > 0:\n        market_null_indecies = add_null_indecies(market_indecies, market_valid_indecies, num_nulls)\n\n    # get index rows\n    market_df_valid = get_indecies(market_df, market_valid_indecies)\n    if num_nulls > 0:\n        market_df_nulls = get_indecies(market_df, market_null_indecies)\n    else:\n        market_df_nulls = 'null'\n    \n    return market_df_valid, market_df_nulls, news_valid_indecies\n\ndef join_market_news(market_df, news_df, nulls=False, num_nulls=0):\n    \n    # convert time to string\n    generalize_time(market_df)\n    generalize_time(news_df)\n    \n    # Fix asset codes (str -> list)\n    news_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_df['assetCodes']))\n    assetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\n    \n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n    \n    if not nulls:\n        market_df, market_df_nulls, news_valid_indecies = partial_groupby(market_df, news_df, df_assetCodes, num_nulls)\n    \n    # create dataframe based on groupby\n    news_col = ['time', 'assetCodes', 'headline', 'audiences', 'subjects'] + sorted(list(news_agg_dict.keys()))\n    news_df_expanded = pd.merge(df_assetCodes, news_df[news_col], left_on='level_0', right_index=True, suffixes=(['','_old']))\n    \n    # check if the columns are in the index\n    if not nulls:\n        news_df_expanded = get_indecies(news_df_expanded, news_valid_indecies)\n    \n    def news_df_feats(x):\n        \n        if x.name == 'headline':\n            return list(x)\n        \n        elif x.name == 'subjects' or x.name == 'audiences':\n            output = []\n            for i in x:\n                # remove all special characters\n                codes = i.strip('{\\',}').replace('\\'','').split(', ')\n                for j in codes:\n                    output.append(j)\n                            \n            return output\n    \n    # groupby time and assetcode\n    news_df_expanded = news_df_expanded.reset_index()\n    news_groupby = news_df_expanded.groupby(['time', 'assetCode'])\n    \n    # get aggregated df\n    news_df_aggregated = news_groupby.agg(news_agg_dict).apply(np.float32).reset_index()\n    news_df_aggregated.columns = ['_'.join(col).strip() for col in news_df_aggregated.columns.values]\n    \n    # get any important string dataframes\n    groupby_news = news_groupby.transform(lambda x: news_df_feats(x))\n    news_df_cat = pd.DataFrame({'headline':groupby_news['headline'],\n                               'subjects':groupby_news['subjects'],\n                               'audiences':groupby_news['audiences']})\n    \n    # apply countvectorizer to get features of subject and audience (commented this out for V59)\n    vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n    for f in ['audiences', 'subjects']:\n        X_codes = vectorizer.fit_transform(news_df_cat[f].tolist())\n        col_names = vectorizer.get_feature_names()\n        X_codes = pd.DataFrame(X_codes.toarray())\n        X_codes.columns = col_names\n        news_df_cat = pd.concat([news_df_cat, X_codes], axis=1)\n        del news_df_cat[f]\n        \n    # merge\n    new_news_df = pd.concat([news_df_aggregated, news_df_cat], axis=1)\n    \n    # cleanup\n    del news_df_aggregated\n    del news_df_cat\n    del news_df\n    \n    # rename columns\n    new_news_df.rename(columns={'time_': 'time', 'assetCode_': 'assetCode'}, inplace=True)\n    new_news_df.set_index(['time', 'assetCode'], inplace=True)\n    \n    # Join with train\n    market_df = market_df.join(new_news_df, on=['time', 'assetCode'])\n    if num_nulls > 0:\n        market_df = pd.concat([market_df, market_df_nulls], sort=False)\n    \n    # replace with null string\n    market_df['headline'] = market_df['headline'].fillna('null')\n\n    return market_df\n\n\n# if there is a joining error, it means that the dataframes have no correlation with each other (solution: increase train dataset)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2be1b6e851438405ca972ddf6fdcc6ea8c97cd28",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "%%time\nX_train = join_market_news(market_train_df, news_train_df, nulls=False, num_nulls=500_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d38b9ffd46a21e96bc55b142452eb7cec432c68"
      },
      "cell_type": "code",
      "source": "X_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67e32e082a729f9aad122d3bcb492a6707e021ea"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "957f29e020695f32628882848eecdf392aa9e9e0"
      },
      "cell_type": "markdown",
      "source": "### Text Processing with Logistic Regression\n\nWe are going to vectorize the headlines and apply logistic regression (labels being binary as to whether the stocks go up or not). In a nutshell, it splits the headlines into individual words, filters out unecessary words to prevent abnormal results, vectorizes it for modelling, and then with the target column provided, we could create a dataframe of coefficients that we could use as a feature in the dataframe! Right now I am just getting the mean of the coefficients in each list of headlines. \n\nNote: \n* May be useful to apply it to ``universe``, and possibly get the sum or standard deviation of the word coefficients?\n* Some key words that should appear in bottom/top?\n    * Free Trade Agreement\n    * Interest Rates\n    * Contracts\n    \nUpdate: We are now going to apply the same code on both ``audiences`` and ``subjects``"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d25c279b7face9bb58eb15a2780a7ec7e9d6b6f8"
      },
      "cell_type": "code",
      "source": "ps = PorterStemmer()\n\n# reuse data\ndef round_scores(x):\n    if x >= 0:\n        return 1\n    else:\n        return 0\n\n# this takes up a lot of time, so apply it when getting coefficients to filter out words.\ndef clean_headlines(headline):\n    \n    # remove numerical and convert to lowercase\n    headline =  re.sub('[^a-zA-Z]',' ',headline)\n    headline = headline.lower()\n    \n    # use stemming to simplify words\n    headline_words = [ps.stem(word) for word in headline.split(' ')]\n    \n    # join sentence back again\n    return ' '.join(headline_words)\n\n# these functions should only go towards the training data only\ndef get_headline_df(X_train, col='headline'):\n    \n    # make sure this is a copy of the given dataframe\n    # remove any headlines with nulls\n    X_train = X_train[X_train[col] != 'null']\n    \n    headlines_lst = []\n    target_lst = []\n    \n    # iter through every headline.\n    for row in range(0,len(X_train.index)):\n        if X_train[col].iloc[row] != 'null':\n            for sentence in X_train[col].iloc[row]:\n                headlines_lst.append(sentence)\n                target_lst.append(round_scores(X_train['returnsOpenNextMktres10'].iloc[row]))\n            \n    # return dataframe\n    return pd.DataFrame({col:pd.Series(headlines_lst), 'returnsOpenNextMktres10':pd.Series(target_lst)})\n    \ndef get_headline(headlines_df):\n    \n    # get headlines as list (use only headline_df produced by get_headline_df)\n    headlines_lst = []\n    for row in range(0,len(headlines_df.index)):\n        if headlines_df.iloc[row] != 'null':\n            headlines_lst.append(clean_headlines(headlines_df.iloc[row]))\n\n    # split headlines to separate words\n    basicvectorizer = CountVectorizer()\n    headlines_vectorized = basicvectorizer.fit_transform(headlines_lst) # error found here (probably in regards to get_headline_df)\n    \n    print(headlines_vectorized.shape)\n    return headlines_vectorized, basicvectorizer\n\ndef headline_mapping(target, headlines_vectored, headline_vectorizer):\n    \n    print(np.asarray(target).shape)\n    headline_model = LogisticRegression()\n    headline_model = headline_model.fit(headlines_vectored, target)\n    \n    # get coefficients\n    basicwords = headline_vectorizer.get_feature_names()\n    basiccoeffs = headline_model.coef_.tolist()[0]\n    coeff_df = pd.DataFrame({'Word' : basicwords, \n                            'Coefficient' : basiccoeffs})\n    \n    # remove stopwords from the dataframe (wait do headlines even have stopwords to begin with?)\n    #coeff_df = coeff_df[coeff_df.Word.isin(stopwords.words('english'))]\n    \n    # convert dataframe to dictionary of coefficients\n    coefficient_dict = dict(zip(coeff_df.Word, coeff_df.Coefficient))\n\n    return coefficient_dict, coeff_df['Coefficient'].mean()\n\n# for predictions\ndef get_coeff_col(X, coeff_dict, coeff_default, col='headline'):\n    \n    def get_coeff(word_lst):\n        \n        # iter through every word\n        coeff_sum = 0\n        for word in word_lst:\n            if word in coeff_dict:\n                coeff_sum += coeff_dict[word]\n            else:\n                coeff_sum += coeff_default\n        \n        # get average coefficient\n        coeff_score = coeff_sum / len(word_lst)\n        return coeff_score\n        \n    basicvectorizer = CountVectorizer()\n    \n    # loop through every item\n    headlines_coeff_lst = []\n    for row in range(0,len(X[col].index)):\n        coeff_score = 0\n        if X[col].iloc[row] == 'null':\n            headlines_coeff_lst.append(np.nan)\n        else:\n            for i in range(0,len(X[col].iloc[row])):\n                curr_str_lst = list(filter(None,clean_headlines(str(X[col].iloc[row][i])).split(' ')))\n                coeff_score += get_coeff(curr_str_lst) / len(curr_str_lst) # get averages here (only applies to headlines)\n        headlines_coeff_lst.append(coeff_score)\n        \n    # merge coefficient frame with main\n    X = X.reset_index()\n    X[col + '_coeff_sum'] = pd.Series(headlines_coeff_lst)\n    \n    return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3fc1bd9717c3e0cd501e7e64758035e47de802f"
      },
      "cell_type": "code",
      "source": "%%time\nheadline_df = get_headline_df(X_train.copy())\ncoefficient_dict, coefficient_default = headline_mapping(headline_df['returnsOpenNextMktres10'],\n                                            *get_headline(headline_df['headline']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eb803ddcfafbf1be8f32af997a99b8a9b5c9b63"
      },
      "cell_type": "code",
      "source": "%%time\n# will be applied to X_test as well\nX_train = get_coeff_col(X_train, coefficient_dict, coefficient_default)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98cbc58e404480a14e611a71186753fd948e5d81"
      },
      "cell_type": "code",
      "source": "X_train[['headline', 'headline_coeff_sum']].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a86041c59933b7ce0685a5c61965024e685cd4a"
      },
      "cell_type": "code",
      "source": "X_train[X_train.headline_coeff_sum.isnull() == False]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "033aca8966e7cc4eb8030dafab9e693b4866457d"
      },
      "cell_type": "markdown",
      "source": "### Clustering\nWe are going to be clustering a few columns together (mainly to see how this will affect our results). "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a269f60c0dc9defa2dfc04c5680b99b5a43b031b"
      },
      "cell_type": "code",
      "source": "def clustering(df):\n\n    def cluster_modelling(features):\n        df_set = df[features]\n        cluster_model = KMeans(n_clusters = 8)\n        cluster_model.fit(df_set)\n        return cluster_model.predict(df_set)\n    \n    # get columns:\n    vol_cols = [f for f in df.columns if f != 'volume' and 'volume' in f]\n    novelty_cols = [f for f in df.columns if 'novelty' in f]\n    \n    # fill nulls\n    cluster_cols = novelty_cols + vol_cols + ['open', 'close']\n    df[cluster_cols] = df[cluster_cols].fillna(0)\n    \n    df['cluster_open_close'] = cluster_modelling(['open', 'close'])\n    df['cluster_volume'] = cluster_modelling(vol_cols)\n    df['cluster_novelty'] = cluster_modelling(novelty_cols)\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "563074af5ae1dab8c5584afef865e8a0d8eef686"
      },
      "cell_type": "code",
      "source": "X_train = clustering(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bdad0b0f9d641b5e028548285cd64747dd5a8ff"
      },
      "cell_type": "markdown",
      "source": "### Extra Features\n\nHere are some basic extra features from other notebooks.\n\nSuggestions:\n* Get market activity per day\n* Moving average of volume?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03532b463b9b4a083aea991708fda777334fbfe2"
      },
      "cell_type": "code",
      "source": "def extra_features(df):\n    \n    # Adding daily difference\n    new_col = df[\"close\"] - df[\"open\"]\n    df.insert(loc=6, column=\"daily_diff\", value=new_col)\n    df['close_to_open'] =  np.abs(df['close'] / df['open'])\n    \n    # get market activity per day\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f83952b1d7852568b8f1d86630d1d8286d90871d"
      },
      "cell_type": "code",
      "source": "X_train = extra_features(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "674bc842d3ab32df25a88facf5a7af2bfdd3254d"
      },
      "cell_type": "markdown",
      "source": "### Cleaning Data\nHere we remove all the excess columns, and encode specific categorical features in preparation of training."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7da8cedce5276b9c145c59ddeeef423df45200b"
      },
      "cell_type": "code",
      "source": "def misc_adjustments(X):\n    \n    # remove list-based columns\n    del_cols = ['headline', 'assetCode']\n    for f in del_cols:\n        del X[f]\n        \n    # get categorical features\n    cols_categorical = ['assetName', 'time']\n        \n    # encode remaining categorical features\n    le = LabelEncoder()\n    X[cols_categorical] = X[cols_categorical].apply(LabelEncoder().fit_transform)\n    \n    return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "007ed8fc74313b6f41789369c765f0293b4d6142"
      },
      "cell_type": "code",
      "source": "X_train = misc_adjustments(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f19d6c49c01767a6b015b7bdca8ee090308978d1"
      },
      "cell_type": "markdown",
      "source": "### Compile X functions into one function\n\nThis will be used when looping through different batches of X_test"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6d8cd8162bb0275302e10bbe5399f87a027fe00"
      },
      "cell_type": "code",
      "source": "def get_X(market_df, news_df):\n    \n    # these are all the functions applied to X_train except for a few\n    news_df = clean_news(news_df)\n    market_df = mean_volume(market_df)\n    market_df = quant_feats(market_df)\n    X_test = join_market_news(market_df, news_df, nulls=True)\n    X_test = get_coeff_col(X_test, coefficient_dict, coefficient_default)\n    X_test = clustering(X_test)\n    X_test = extra_features(X_test)\n    X_test = misc_adjustments(X_test)\n    \n    return X_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "055fef0b89c100b52ec1e6405ca5e3dde68314e8"
      },
      "cell_type": "markdown",
      "source": "#### Resulting Dataframe \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ac267c074efb8d7e16620b52320d23ec4eb8b01"
      },
      "cell_type": "code",
      "source": "X_train.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbf14470186656cb663be8be4dcbe6e8d61aea71"
      },
      "cell_type": "code",
      "source": "X_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b3fcb764e84a394290fc45be45a5e85a7296c61"
      },
      "cell_type": "markdown",
      "source": "### Using LGBM For Modelling\nWhat could go wrong?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99b16bf2669371fa6a68ad0582deb023a82971f3"
      },
      "cell_type": "code",
      "source": "def fixed_train_test_split(X, y, train_size):\n    \n    # round train size\n    train_size = int(train_size * len(X))\n    \n    # split data\n    X_train, y_train = X[train_size:], y[train_size:]\n    X_valid, y_valid = X[:train_size], y[:train_size]\n    \n    return X_train, y_train, X_valid, y_valid\n\ndef set_data(X_train):\n    \n    # get X and Y\n    y_train = X_train['returnsOpenNextMktres10']\n    del X_train['returnsOpenNextMktres10'], X_train['universe']\n    \n    # split data (for cross validation)\n    x1, y1, x2, y2 = fixed_train_test_split(X_train, \n                                              y_train, \n                                              0.8)\n    \n    # set lgbm dataframes\n    dtrain = lgb.Dataset(x1, label=y1)\n    dvalid = lgb.Dataset(x2, label=y2)\n    \n    return dtrain, dvalid\n    \ndef lgbm_training(X_train):\n    \n    # set model and parameters\n    params = {'learning_rate': 0.02, \n              'boosting': 'gbdt', \n              'objective': 'regression_l1', \n              'seed': 573,\n            'sub_feature': 0.7,\n            'num_leaves': 60,\n            'min_data': 100,\n            'verbose': -1}\n    \n    # get x and y values\n    dtrain, dvalid = set_data(X_train)\n    \n    # train data\n    lgb_model = lgb.train(params, \n                            dtrain, \n                            1000, \n                            valid_sets=(dvalid,), \n                            verbose_eval=25, \n                            early_stopping_rounds=100)\n    \n    return lgb_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70387e36cd94a6b79edbbab1bb0dde8f53776300"
      },
      "cell_type": "code",
      "source": "lgb_model = lgbm_training(X_train.copy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3762332a038544f7ed3c9ba5acd5c6fd114b946f"
      },
      "cell_type": "markdown",
      "source": "### Making Predictions\n\nNow the difference between the training and test data would be these two columns,  ``['returnsOpenNextMktres10', 'universe']``. We will be trying to predict ``returnsOpenNextMktres10`` and using that as the ``confidenceValue``."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1fb894dd1216d1dbf6e48d15d336a8fd1847067"
      },
      "cell_type": "code",
      "source": "%%time\n\ndef make_predictions(market_obs_df, news_obs_df):\n    \n    # predict using given model\n    X_test = get_X(market_obs_df, news_obs_df)\n    prediction_values = np.clip(lgb_model.predict(X_test), -1, 1)\n\n    return prediction_values\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days(): # Looping over days from start of 2017 to 2019-07-15\n    \n    # make predictions\n    predictions_template_df['confidenceValue'] = make_predictions(market_obs_df, news_obs_df)\n    \n    # save predictions\n    env.predict(predictions_template_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9c3668a134db935e6a7e266a15a47a7292f537e"
      },
      "cell_type": "markdown",
      "source": "### Export Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02e6a605c9ca804310fbc788b465c226ddfea533"
      },
      "cell_type": "code",
      "source": "# exports csv\nenv.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d8f34f002aa0dc6efda2caf73a8fd5e2724515b"
      },
      "cell_type": "markdown",
      "source": "### References:\n* [Getting Started - DJ Sterling](https://www.kaggle.com/dster/two-sigma-news-official-getting-started-kernel)\n* [a simple model - Bruno G. do Amaral](https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-data)\n* [LGBM Model - the1owl](https://www.kaggle.com/the1owl/my-two-sigma-cents-only)\n* [Headline Processing - Andrew Gel√©](https://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit)\n* [Feature engineering - Andrew Lukyanenko](https://www.kaggle.com/artgor/eda-feature-engineering-and-everything)\n* [Basic Text Processing - akatsuki06](https://www.kaggle.com/akatsuki06/basic-text-processing-cleaning-the-description)\n* [The fallacy of encoding assetCode - marketneutral](https://www.kaggle.com/marketneutral/the-fallacy-of-encoding-assetcode)\n* [Market Data NN Baseline  - dieter](https://www.kaggle.com/christofhenkel/market-data-nn-baseline)\n* [Aantonova Features - aantonova](https://www.kaggle.com/aantonova/797-lgbm-and-bayesian-optimization/notebook)\n* [Simple quant features using python - YouHan Lee](https://www.kaggle.com/youhanlee/simple-quant-features-using-python)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}